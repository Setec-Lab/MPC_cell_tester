{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e9a24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Expand jupyter cell to complete view in high resolution monitor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45697c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical features: \n",
      "\n",
      "There are 66 numerical features: \n",
      "\n",
      "Cantidad de muestras originales de entrenamiento\n",
      "0    134568\n",
      "1      1816\n",
      "Name: Y, dtype: int64\n",
      "Cantidad de muestras reservadas para testeo de clase 1\n",
      "320\n",
      "Cantidad de muestras por clase después de realizar oversampling\n",
      "0    134568\n",
      "1     20185\n",
      "Name: Y, dtype: int64\n",
      "Cantidad de muestras por clase después de realizar undersampling\n",
      "0    67283\n",
      "1    20185\n",
      "Name: Y, dtype: int64\n",
      "Split de datos de entrenamiento y testeo\n",
      "\tTrain:  61227\n",
      "\tTest:  26241\n",
      "Split de datos de entrenamiento y testeo al agregar datos de clase 1 nuevos\n",
      "\tTrain:  61227\n",
      "\tTest:  26561\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d \n",
    "# Pipeline and gridsearch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "import imblearn\n",
    "from imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler, OneSidedSelection,  NeighbourhoodCleaningRule, InstanceHardnessThreshold, AllKNN, NearMiss)\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTEN, SMOTE, ADASYN, BorderlineSMOTE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import statsmodels.api as sm\n",
    "import pylab\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "                                                                            # df refers to data frame\n",
    "                                                                            # add here the new data extracted for validation purposes\n",
    "#df_raw_test = pd.read_csv('C:\\\\Users\\\\kevinmor\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Full Time\\\\AFE AI Assesment\\\\CSE\\\\testdataset.csv')\n",
    "                                                                            # volume raw data for further prediction model testing purposes\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\kevinmor\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Full Time\\\\AFE AI Assesment\\\\CSE\\\\afe_cold_5430_train_minus15.csv')\n",
    "                                                                            # dataset with unseen class 1 examples\n",
    "df_test = pd.read_csv('C:\\\\Users\\\\kevinmor\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Full Time\\\\AFE AI Assesment\\\\CSE\\\\afe_cold_5430_test_15.csv')\n",
    "\n",
    "                                                                            # there is pending to automate column removal, for example when one column has just one value (it does not contribute with valuable information)\n",
    "\n",
    "                                                                            # split features and output of test dataset\n",
    "y_t = df_test.Y\n",
    "X_t = df_test.drop('Y',axis=1)\n",
    "                                                                            # split features and output of training dataset\n",
    "y = df.Y\n",
    "X = df.drop('Y',axis=1)\n",
    "\n",
    "                                                                            # X and y contain original dataset\n",
    "                                                                            # confirm that there are not categorical features          \n",
    "categorical_features = X.select_dtypes(exclude='number').columns.tolist()\n",
    "print(f'There are {len(categorical_features)} categorical features:', '\\n')\n",
    "                                                                            # obtain numerical features\n",
    "numerical_features = X.select_dtypes(include='number').columns.tolist()\n",
    "print(f'There are {len(numerical_features)} numerical features:', '\\n')\n",
    "#print(numerical_features)\n",
    "                                                                            # X_cc and y_cc will contain preprocessed data\n",
    "                                                                            # u refers to unprocessed \n",
    "X_ccu, y_cc = X, y\n",
    "print(\"Cantidad de muestras originales de entrenamiento\")\n",
    "print(df['Y'].value_counts())  \n",
    "\n",
    "\n",
    "print(\"Cantidad de muestras reservadas para testeo de clase 1\")\n",
    "print(len(X_t))\n",
    "\n",
    "                                                                            # oversampling or boostering    \n",
    "                                                                            # https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "                                                                            # the minority class would have, in this case, the sampling_strategy*100 percentaje of the majority class examples\n",
    "                                                                            # https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler.fit_resample\n",
    "                                                                        \n",
    "# Minority class proportion\n",
    "ONm= 15\n",
    "# Mayority class proportion\n",
    "ONM= 100\n",
    "ratioOverSampling= ONm/ONM\n",
    "\n",
    "# SMOTE and ADASYN generate new samples in by interpolation\n",
    "# When dealing with a mixed of continuous and categorical features, SMOTENC is the only method which can handle this case.\n",
    "# ADASYN will focus on the samples which are difficult to classify with a nearest-neighbors rule while regular SMOTE will not make any distinction\n",
    "# https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.html#imblearn.over_sampling.SMOTEN\n",
    "# number of nearest neighbours to used to construct synthetic samples\n",
    "# https://imbalanced-learn.org/stable/auto_examples/over-sampling/plot_comparison_over_sampling.html\n",
    "oversamplers = [RandomOverSampler(sampling_strategy=ratioOverSampling), SMOTEN(random_state=17, sampling_strategy=ratioOverSampling, n_jobs=-1, k_neighbors=5), BorderlineSMOTE(random_state=0, kind=\"borderline-1\") ]\n",
    "oversample = oversamplers[0]\n",
    "                                                                            # fit and apply the transform\n",
    "X_ccu, y_cc = oversample.fit_resample(X_ccu, y_cc)\n",
    "print(\"Cantidad de muestras por clase después de realizar oversampling\")\n",
    "print(y_cc.value_counts())\n",
    "\n",
    "#Under-sampling: Cluster Centroids\n",
    "# Minority class proportion\n",
    "Nm= 30\n",
    "# Mayority class proportion\n",
    "NM= 100\n",
    "ratioUnderSampling= Nm/NM\n",
    "\n",
    "# Compare undersamplers\n",
    "# https://imbalanced-learn.org/stable/auto_examples/under-sampling/plot_comparison_under_sampling.html#sphx-glr-auto-examples-under-sampling-plot-comparison-under-sampling-py\n",
    "samplers= [ClusterCentroids(sampling_strategy=ratioUnderSampling), \n",
    "           InstanceHardnessThreshold(estimator=LogisticRegression(),random_state=0),\n",
    "           OneSidedSelection(random_state=0), \n",
    "           NeighbourhoodCleaningRule(),\n",
    "           AllKNN(allow_minority=True),\n",
    "           NearMiss(version=1, sampling_strategy=ratioUnderSampling, n_jobs=-1), \n",
    "           NearMiss(version=2, sampling_strategy=ratioUnderSampling, n_jobs=-1), \n",
    "           NearMiss(version=3, n_neighbors_ver3=3, n_jobs=-1)]\n",
    "                                                                            # undersampling\n",
    "                                                                            # https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "#undersample = RandomUnderSampler(sampling_strategy=1)\n",
    "                                                                            # fit and apply the transform\n",
    "#X_cc, y_cc = undersample.fit_resample(X_cc, y_cc)\n",
    "cc = samplers[5]\n",
    "X_ccu, y_cc = cc.fit_resample(X_ccu, y_cc)\n",
    "                                                                            # summarize class distribution\n",
    "print(\"Cantidad de muestras por clase después de realizar undersampling\")\n",
    "print(y_cc.value_counts())\n",
    "                                                                            # Combining over/under-sampling can result in improved overall performance compared to performing one or the other techniques in isolation.\n",
    "                                                                            # split data into train and test subsets\n",
    "                                                                            # altough class 0 test examples are new to the AI models that are going to be trained, class 1 examples are not new, this because of the oversampling strategy followed.\n",
    "                                                                            # due to the reproduction of class 1 examples, at test dataset, there are \"seen\" examples for the neuronal networks, so there are needed real new examples, this is the\n",
    "                                                                            # intend of X_t examples.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ccu, y_cc, train_size=0.7, random_state=17)\n",
    "print(\"Split de datos de entrenamiento y testeo\")\n",
    "print(\"\\tTrain: \", len(X_train))\n",
    "print(\"\\tTest: \", len(X_test))\n",
    "                                                                            # add 321 (15 % of original class 1) unseen class 1 examples\n",
    "X_test=np.append(X_test, X_t, axis=0)\n",
    "y_test=np.append(y_test, y_t, axis=0)\n",
    "print(\"Split de datos de entrenamiento y testeo al agregar datos de clase 1 nuevos\")\n",
    "print(\"\\tTrain: \", len(X_train))\n",
    "print(\"\\tTest: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6f1cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5,  0. ,  0. , ...,  0.8,  0.5,  0. ],\n",
       "       [-0.5,  0. ,  0. , ...,  1.4,  0. ,  0. ],\n",
       "       [ 0.5,  0. ,  1. , ..., -0.6,  2.5,  0. ],\n",
       "       ...,\n",
       "       [ 0. ,  0. ,  0. , ..., -0.4, -0.5,  0. ],\n",
       "       [-0.5,  0. ,  0. , ...,  0. , -0.5,  0. ],\n",
       "       [ 1. ,  0. ,  1. , ...,  0.4,  2.5,  0. ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                                            # https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d\n",
    "                                                            # Define preprocessing for data\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('scale', RobustScaler())\n",
    "    ])\n",
    "                                                            # pipeline objects have fit and transform methods\n",
    "numeric_pipeline.fit_transform(X_train.select_dtypes(include='number'))\n",
    "                                                            # define scaler for numerical features                                                            \n",
    "full_processor = ColumnTransformer(transformers=[\n",
    "    ('number', numeric_pipeline, numerical_features)\n",
    "    ])\n",
    "# apply transformation\n",
    "full_processor.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV for tunning model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\n",
    "# https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "# https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    "# https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
    "# https://xgboost-clone.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "\n",
    "#  scale_pos_weight is the ratio of number of negative class to the positive class (0.7)\n",
    "# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n",
    "# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n",
    "# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n",
    "# A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n",
    "\n",
    "# 3888 experiments\n",
    "param_grid = {'gamma': [0.1, 0.25, 0.35],\n",
    "              'eta': [0.15, 0.25, 0.38],\n",
    "              'max_depth': [35, 40, 45],\n",
    "              'n_estimators': [50, 100,200],\n",
    "              'reg_alpha': [0.05, 0.1, 0.15],\n",
    "              'reg_lambda': [0.05, 0.1, 0.15],\n",
    "              'colsample_bytree': [0.90, 0.95, 1],\n",
    "              'colsample_bylevel': [0.95, 1],\n",
    "              'min_child_weight': [5, 10],\n",
    "              'scale_pos_weight': [0.72, 0.75, 0.8]}\n",
    "\n",
    "\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "# Init Grid Search\n",
    "grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"f1\")\n",
    "\n",
    "# Fit\n",
    "_ = grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('Best score:', abs(grid_cv.best_score_))\n",
    "print('Best alpha:', grid_cv.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196ff11f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_15092/739137773.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kevinmor\\AppData\\Local\\Temp/ipykernel_15092/739137773.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    >>> preds = final_lasso_pipe.predict(X_valid)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# fill with the best parameters found\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "final_lasso_pipe = Pipeline(steps=[\n",
    "    ('preprocess', full_processor),\n",
    "    ('model', lasso)\n",
    "])\n",
    "\n",
    "_ = final_lasso_pipe.fit(X_train, y_train)\n",
    ">>> preds = final_lasso_pipe.predict(X_valid)\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "resultXGB = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "#accuracy_score(y_test, preds)\n",
    "print(\"Accuracy for XGBoost: \" + str(accuracy_score(resultXGB, y_test)))\n",
    "print(\"F1_Score for XGBoost: \" + str(f1_score(resultXGB, y_test)))\n",
    "print(\"Precision_Score for XGBoost: \" + str(precision_score(resultXGB, y_test)))\n",
    "print(\"Recall_Score for XGBoost: \" + str(recall_score(resultXGB, y_test)))\n",
    "\n",
    "cm = confusion_matrix(y_test, resultXGB)\n",
    "%matplotlib inline\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c97741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevinmor\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:27] WARNING: D:\\bld\\xgboost-split_1643227225381\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for XGBoost: 0.9370129136704191\n",
      "F1_Score for XGBoost: 0.8545090877467607\n",
      "Precision_Score for XGBoost: 0.7640746500777605\n",
      "Recall_Score for XGBoost: 0.9692246991517065\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390\n",
    "#https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97\n",
    "# Pipeline: https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d\n",
    "\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "resultXGB = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "#accuracy_score(y_test, preds)\n",
    "print(\"Accuracy for XGBoost: \" + str(accuracy_score(resultXGB, y_test)))\n",
    "print(\"F1_Score for XGBoost: \" + str(f1_score(resultXGB, y_test)))\n",
    "print(\"Precision_Score for XGBoost: \" + str(precision_score(resultXGB, y_test)))\n",
    "print(\"Recall_Score for XGBoost: \" + str(recall_score(resultXGB, y_test)))\n",
    "\n",
    "cm = confusion_matrix(y_test, resultXGB)\n",
    "%matplotlib inline\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
